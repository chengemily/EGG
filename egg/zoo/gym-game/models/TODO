Experiments

1. Student no teacher
    [ ] Optimizer
        [ ] Adam
        [ ] RMSProp
    [ ] Scheduler
        [ ] Reduce LR on Plateau
            - lr = 0.0005, rmsprop converges. Other lrs tried did not converge (seems sensitive to lr)
            -
        [x] CyclicLR
            - Bad results (did not converge)
        [ ] CosineAnnealing
    [ ] Learning rate
        [ ] Tried 5e-05, 1e-04, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1
        [ ] 0.0005 <-> 0.05 do alright. 0.1 is too much in all trials and loss quickly diverges
